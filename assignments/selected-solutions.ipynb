{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c52756",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a217f78",
   "metadata": {},
   "source": [
    "## Exercise 2. Regression\n",
    "### EXERCISE1-TASK1 \n",
    "Many phenomena associated with complex networks exhibit heavy-tailed distributions. For example, the number of followers a user has on the social media platform formerly known as Twitter has been characterized by a power law distribution:\n",
    "\n",
    "$\n",
    "y = \\alpha x ^\\beta\n",
    "$\n",
    "\n",
    "where $x$ is the number of followers, $y$ is the frequency of occurrence, and $\\alpha$ and $\\beta$ are parameters. \n",
    "\n",
    "**[10 marks]** Show that the estimation of parameters $\\alpha$ and $\\beta$ is a convex optimization problem, and provide a domain for which your assertion holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca689502",
   "metadata": {},
   "source": [
    "##### [ SOLUTION BLOCK: Write your response here ]\n",
    "\n",
    "While the equation is nonlinear in terms of the parameters $\\alpha$ and $\\beta$, we can transform it to a linear model by using a logarithmic transformation:\n",
    "\n",
    "$\n",
    "\\textrm{log} y = \\textrm{log}(\\alpha x^{\\beta}) = \\textrm{log} \\alpha + \\beta \\textrm{log} x\n",
    "$\n",
    "\n",
    "Letting $y' = \\textrm{log} y$, $x' \\textrm{log} x$, and $\\alpha' = \\textrm{log}\\alpha$ the model can be rewitten as:\n",
    "\n",
    "$\n",
    "y' = \\beta x' + \\alpha'\n",
    "$\n",
    "\n",
    "Which is linear in its parameters. We can therefore use the least-squares regression to estimate the parameters $\\alpha'$ and $\\beta$, given (log-transformed) samples $x'_i$ and $y'_i$:\n",
    "\n",
    "$\n",
    "\\textrm{min}_{\\alpha',\\beta} \\sum_{i=1}^n \\| y'_i - (\\beta'x'_i + \\alpha')\\|^2\n",
    "$\n",
    "\n",
    "This is convex as it minimizes a sum of squares, which is a convex function. This is true provided the logarithm is defined, restricting the original $x$ and $y$ to the halfspace of positive numbers. Similarly, as $\\alpha = e^{\\alpha'}$, $\\alpha$ must be strictly positive.\n",
    "\n",
    "Therefore, the domain for which this optimization problem is convex is:\n",
    "$x, y, \\alpha \\forall \\mathbb{R}_{++}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e40ce",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b33998",
   "metadata": {},
   "source": [
    "## Exercise 1. Maximum Likelihood Estimation\n",
    "\n",
    "The probability density function of a Beta distribution is given by:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\eta},\n",
    "$$\n",
    "\n",
    "where $x$ is the value of the random variable, $\\alpha$ and $\\beta$ are shape parameters, and $\\eta$ is a normalizing constant. Suppose that a dataset $\\boldsymbol{x} = \\{ x_1, x_2, \\ldots, x_n \\}$ is given, where each observation $x_i$ follows a Beta distribution. It is known that the relationship between the shape parameters is:\n",
    "\n",
    "$$\n",
    "\\beta = \\alpha^2\n",
    "$$\n",
    "\n",
    "### **EXERCISE1-TASK1: [10 marks]**\n",
    "\n",
    "Derive the maximum likelihood estimate of the shape parameter $\\beta$. That is, starting with the likelihood function, $\\mathcal{L}(\\alpha, \\beta \\mid \\boldsymbol{x})$, provide an analytical expression to compute $\\beta$, given some $\\alpha$.\n",
    "The normalizing constant $\\eta$ can be omitted during differentiation, as it does not affect the location of the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa3a90",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\alpha,\\beta|\\boldsymbol{x}) &= \\prod_{i=1}^n x_i^{\\alpha-1}(1-x_1)\n",
    "^{\\beta-1}\\\\\n",
    "\\textrm{log}\\mathcal{L}(\\alpha,\\beta|\\boldsymbol{x}) &= (\\alpha-1)\\sum_{i=1}^n\\textrm{log}x_i +(\\beta-1)\\sum_{i=1}^n\\textrm{log}(1-x_i)\\\\\n",
    "\\textrm{Substituting $\\beta = \\alpha^2$}&\\\\\n",
    "\\textrm{log}\\mathcal{L}(\\alpha|\\boldsymbol{x}) &= (\\alpha-1)\\sum_{i=1}^n\\textrm{log}x_i +(\\alpha^2-1)\\sum_{i=1}^n\\textrm{log}(1-x_i)\\\\\n",
    "\\frac{\\partial}{\\partial \\alpha}\\textrm{log}\\mathcal{L}(\\alpha|\\boldsymbol{x}) &= \\sum_{i=1}^n\\textrm{log}x_i + 2\\alpha\\sum_{i=1}^n\\textrm{log}(1-x_i) = 0\\\\\n",
    "\\implies \\alpha &= -\\frac{\\sum_{i=1}^n\\textrm{log}x_i}{2\\sum_{i=1}^n\\textrm{log}(1-x_i)}\\\\\n",
    "\\implies \\beta &= \\Bigg(-\\frac{\\sum_{i=1}^n\\textrm{log}x_i}{2\\sum_{i=1}^n\\textrm{log}(1-x_i)}\\Bigg)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ignoring the normalizing constant creates an issue in the expression for the maximum likelihood estimate of $\\alpha$. The Beta distribution is defined over $[0,1]$, so $0 < x_i < 1 \\; \\forall i$. Consequently, both $\\sum_{i=1}^n\\textrm{log}x_i$ and $\\sum_{i=1}^n\\textrm{log}(1-x_i)$ are negative values. We will thus obtain a positive quotient, which we then mulitply by a negative scaling factor, resulting in a negative MLE for $\\alpha$. This violates the parametrization of the Beta distribution.\n",
    "This violation was due to ignoring the normalizing \"constant\". This is not a constant but in fact a function of $\\alpha$ and $\\beta$. It therefore has an effect on the position of the maximum, contrary to what the question suggested. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51be74",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6fcc0",
   "metadata": {},
   "source": [
    "## Exercise 2. Bias-Variance Decomposition\n",
    "In class, we analyzed a regression problem of predicting a continuous random variable $Y \\in \\mathbb{R}$ which is an unknown function $f: \\;\\; \\mathbb{R}^d \\to \\mathbb{R}$ of $X \\in \\mathbb{R}^d$ corrupted by Gaussian noise $\\epsilon \\sim \\mathcal{N}(\\mu = 0, \\sigma_{\\epsilon}^2)$\n",
    "\\begin{align*}\n",
    "Y &= f(X) + \\epsilon\\\\\n",
    "\\end{align*}\n",
    "and saw how the implications of decomposing expected error on the test dataset into the three components that we labelled irreducible error, squared bias, and variance.\n",
    "\n",
    "### **EXERCISE2-TASK1: [10 marks]**\n",
    "For this regression setup, show that the expectation of the squared error for some new test input $x_t$:\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\mathbb{E}\\left[ (Y - \\hat{f}(x_t))^2 \\mid X = x_t \\right] \\\\\n",
    "\\end{align*}\n",
    "May be expressed as\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\sigma_{\\epsilon}^2 + \\left[ \\mathbb{E}[\\hat{f}(x_t)] - f(x_t) \\right]^2 + \\mathbb{E}\\left[ \\left( \\hat{f}(x_t) - \\mathbb{E}[\\hat{f}(x_t)] \\right)^2 \\right].\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\hat f(x)$ denotes our approximated function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9afd25",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "% begin\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\mathbb{E}\\left[ (Y - \\hat{f}(x_t))^2 \\mid X = x_t \\right] \\\\\n",
    "&= \\mathbb{E}\\left[ ( {f}(x_t) + \\epsilon_t - \\hat{f}(x_t))^2 \\right] \\\\\n",
    "\\end{align*}\n",
    "We can next introduce the expected value of the prediction for $x_t$, $\\mathbb{E}[\\hat f(x_t)]$, inside the squaring operation, and then rearrange and subsequently group terms:\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\mathbb{E}\\left[( {f}(x_t) + \\epsilon_t - \\hat{f}(x_t) + \\mathbb{E}[\\hat f(x_t)] - \\mathbb{E}[\\hat f(x_t)])^2 \\right] \\\\\n",
    "&= \\mathbb{E}\\left[( \\epsilon_t + {f}(x_t) - \\mathbb{E}[\\hat f(x_t)] + \\mathbb{E}[\\hat f(x_t)] - \\hat{f}(x_t)  )^2 \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ \\left( \\left( \\epsilon_t +f(x_t) - \\mathbb{E}[\\hat{f}(x_t)] \\right) + \\left( \\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t) \\right) \\right)^2 \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We can now expand the squared terms using $(a+b)^2 = a^2 + 2ab + b^2$, recognizing $a$ as $\\epsilon_t + f(x_t) - \\mathbb{E}[\\hat f(x_t)]$ and $b$ as $\\mathbb{E}[\\hat f(x_t)] - \\hat{f}(x_t)$:\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\mathbb{E} \\left[\n",
    "(\\epsilon_t + f(x_t) - \\mathbb{E}[\\hat{f}(x_t)])^2 \n",
    "+ 2 (\\epsilon_t + f(x_t) - \\mathbb{E}[\\hat{f}(x_t)]) (\\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t)) \n",
    "+ (\\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t))^2 \n",
    "\\right]\n",
    "\\end{align*}\n",
    "The first term can be expanded further, using $(a+b)^2 = a^2 + 2ab + b^2$ again, this time recognizing $a$ as $\\epsilon_t$ and $b$ as $f(x_t) - \\mathbb{E}[\\hat f(x_t)]$\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\mathbb{E} \\left[\n",
    "\\epsilon_t^2 \n",
    "+ 2\\epsilon_t(f(x_t) - \\mathbb{E}[\\hat{f}(x_t)]) \n",
    "+ (f(x_t) - \\mathbb{E}[\\hat{f}(x_t)])^2 \n",
    "+ 2 (\\epsilon_t + f(x_t) - \\mathbb{E}[\\hat{f}(x_t)]) (\\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t)) \n",
    "+ (\\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t))^2 \n",
    "\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Leveraging the linearity of expections, we can now apply the outermost expectation operation to each of the five terms inside the bracket:\n",
    "\n",
    "- The first term becomes $\\mathbb{E}[\\epsilon_t^2]$, which reduces to the variance of the noise, $\\sigma_{\\epsilon}^2$, as $\\epsilon$ follows a normal distribution with zero mean.\n",
    "\n",
    "- Taking the expectation of the second term, we obtain $\\mathbb{E}[2\\epsilon_t(f(x_t) - \\mathbb{E}[\\hat{f}(x_t)])]$. This term is zero as $\\mathbb{E}[\\epsilon]$ = 0.\n",
    "\n",
    "- The third term, $(f(x_t) - \\mathbb{E}[\\hat{f}(x_t)])^2$ remains as is, as it is independent of the test dataset.\n",
    "\n",
    "- Taking the expectation of the fourth term, we obtain $\\mathbb{E}[2 (\\epsilon_t + f(x_t) - \\mathbb{E}[\\hat{f}(x_t)]) (\\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t))]$ which is zero as the fluctuations around the average prediction have zero mean. \n",
    "\n",
    "- Finally, the expectation of the fifth term is given by $\\mathbb{E}[(\\mathbb{E}[\\hat{f}(x_t)] - \\hat{f}(x_t))^2]$. \n",
    "\n",
    "Thus we are left with terms 1, 3, and 5, which simplify the expression for $\\textrm{Err}(x_t)$ to\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\sigma_{\\epsilon}^2 + \\left[ \\mathbb{E}[\\hat{f}(x_t)] - f(x_t) \\right]^2 + \\mathbb{E}\\left[ \\left( \\hat{f}(x_t) - \\mathbb{E}[\\hat{f}(x_t)] \\right)^2 \\right].\n",
    "\\end{align*}\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b3e89",
   "metadata": {},
   "source": [
    "## Exercise 3. Bagging with Correlated Samples\n",
    "(Adapted from Exercise 15.1 in *Elements of Statistical Learning*) A critical assumption underpinning bootstrap aggregation is that pairwise correlations between model errors is low. However, independent bootstrap realizations will not be perfectly decorrelated, and so the actual variance reduction achieved in practice will be much less. In this task you will explore the variance reduction when samples are not independent, that is, they are correlated. \n",
    "\n",
    "### **EXERCISE3-TASK1: [10 marks]**\n",
    "Suppose you have a set of $Bn$ samples $x_i$ drawn from a normal distribution with some mean $\\mu$ and variance $\\sigma^2$, that is $x_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$, and your aim is to average over $B$ bootstrapped estimates of the mean. In class, we found that when samples were independent, the variance of the estimate of the mean was $\\frac{1}{B}\\sigma^2$. Now however, assume that any two samples $x_i$ and $x_j$ are correlated with some correlation coefficient $\\rho > 0$. \n",
    "\n",
    "Show that the variance of the estimate of the mean is given by:\n",
    "\\begin{align*}\n",
    "\\rho \\sigma^2 + \\frac{1-p}{B}\\sigma^2\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3cc5b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "We are told to assume that $ x_t \\sim N(m, \\sigma^2) $ for all $ i $ and that $ x_t $ and $ x_j $ are correlated with a correlation coefficient of $ \\rho $. We first compute some expectation of powers of $ x_t $. \n",
    "\n",
    "First, recall from probability that the variance in terms of the moments is given by\n",
    "\\begin{align*}\n",
    "E[x^2] = \\text{Var}(X) + E[X]^2.\n",
    "\\end{align*}\n",
    "Next, the definition of the correlation coefficient $ \\rho $ gives us\n",
    "\\begin{align*}\n",
    "\\frac{1}{\\sigma^2} E[(x_t - m)(x_j - m)] = \\rho > 0,\n",
    "\\end{align*}\n",
    "for $ i \\neq j $. We can expand the above expectation to get\n",
    "\\begin{align*}\n",
    "E[x_t x_j] = \\rho \\sigma^2 + m^2.\n",
    "\\end{align*}\n",
    "Thus, we have shown that\n",
    "\\begin{align*}\n",
    "E[x_t] &= m, \\\\\n",
    "E[x_t x_j] &= \\begin{cases} \n",
    "\\rho \\sigma^2 + m^2 & i \\neq j, \\\\\n",
    "\\sigma^2 + m^2 & i = j.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "The variance of the estimate of the mean is now given by\n",
    "\\begin{align*}\n",
    "\\text{Var} \\left( \\frac{1}{B} \\sum_{i=1}^B x_t \\right) &= \\frac{1}{B^2} \\text{Var} \\left( \\sum_{i=1}^B x_t \\right) \\\\\n",
    "&= \\frac{1}{B^2} E \\left[ \\left( \\sum_{i=1}^B x_t \\right)^2 \\right] - E \\left[ \\sum_{i=1}^B x_t \\right]^2.\n",
    "\\end{align*}\n",
    "The second expectation is easy to evaluate:\n",
    "\\begin{align*}\n",
    "E \\left( \\sum_{i=1}^B x_t \\right) = \\sum_{i=1}^B E[x_t] = Bm,\n",
    "\\end{align*}\n",
    "as $ E[x_t] = m $ for all $ i $. For the first expectation, note that\n",
    "\\begin{align*}\n",
    "\\left( \\sum_{i=1}^B x_t \\right)^2 = \\sum_{t,j=1}^B x_t x_j.\n",
    "\\end{align*}\n",
    "Thus, we have\n",
    "\\begin{align*}\n",
    "E \\left[ \\left( \\sum_{i=1}^B x_t \\right)^2 \\right] &= \\sum_{t,j=1}^B E[x_t x_j] \\\\\n",
    "&= B E[x_t^2] + (B^2 - B) E[x_t x_j] \\\\\n",
    "&= B(\\sigma^2 + m^2) + (B^2 - B)(\\rho \\sigma^2 + m^2) \\\\\n",
    "&= B \\sigma^2 + B^2 \\rho \\sigma^2 + B m^2 + B^2 m^2 - B \\rho \\sigma^2 - B m^2.\n",
    "\\end{align*}\n",
    "\n",
    "With this expression in Equation 154, we can now compute $ \\text{Var} \\left( \\frac{1}{B} \\sum_{i=1}^B x_t \\right) $ and find\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var} \\left( \\frac{1}{B} \\sum_{i=1}^B x_t \\right) &= \\frac{\\sigma^2}{B} + \\rho \\sigma^2 - \\frac{\\rho \\sigma^2}{B}\\\\\n",
    "&= \\rho \\sigma^2 + \\frac{1 - \\rho}{B} \\sigma^2.\n",
    "\\end{align*}\n",
    "\n",
    "Which completes the proof.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4561c2",
   "metadata": {},
   "source": [
    "## Exercise 5. Multilayer Perceptrons\n",
    "\n",
    "\n",
    "### EXERCISE5-TASK1 [10 marks]:\n",
    "Compute the derivative of the Mish activation function with respect to $x$\n",
    "\\begin{align}\n",
    "\\textrm{mish}(x) = x \\cdot \\textrm{tanh}( \\textrm{log}(1+e^x) )\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120bb79",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "We may recognize the $\\textrm{mish}$ function as a product of two terms: $x$ and $\\textrm{tanh}(\\textrm{log}(1+e^x))$. We will need to use the product rule to compute the derivative of this activation function as a whole, and the second term will involve the chain rule, as we see that $x$ is nested within three functions. \n",
    "\n",
    "Defining $w = \\textrm{tanh}(y)$, $y = \\textrm{log}(z)$ and $z = 1+e^x$, we can compute\n",
    "\\begin{align*}\n",
    "\\frac{\\partial w}{\\partial x} = \\frac{\\partial w}{\\partial y}\\frac{\\partial y}{\\partial z}\\frac{\\partial z}{\\partial x}\n",
    "\\end{align*}\n",
    "\n",
    "To compute $\\frac{\\partial}{\\partial y}\\textrm{tanh}(y)$, we can make use of the identity\n",
    "\\begin{align*}\n",
    "\\textrm{tanh}(y) = \\frac{e^y - e^{-y}}{e^y + e^{-y}}\n",
    "\\end{align*}\n",
    "\n",
    "enabling us to leverage the quotient rule\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}\\Bigg( \\frac{f(y)}{g(y)} \\Bigg) = \\frac{f'(y)g(y)-g'(y)f(y)}{g(y)^2},\n",
    "\\end{align*}\n",
    "\n",
    "together with the identities $\\frac{\\partial}{\\partial y}e^y = e^y$ and $\\frac{\\partial}{\\partial x}e^{-y} = -e^{-y}$ to obtain\n",
    "\\begin{align*}\n",
    "\\frac{d}{dy} \\tanh(y) &= \\frac{(e^y + e^{-y})(e^y - e^{-y}) - (e^y - e^{-y})(e^y - e^{-y})}{(e^y + e^{-y})^2} \\\\\n",
    "&= 1 - \\frac{(e^y - e^{-y})^2}{(e^y + e^{-y})^2} = 1 - \\tanh^2(y)\n",
    "\\end{align*}\n",
    "\n",
    "We can next compute the remaining derivatives, which yield $\\frac{\\partial}{\\partial z}\\textrm{log}(z) = \\frac{1}{z}$, and $\\frac{\\partial}{\\partial x}(1+e^x) = e^x$. Combining terms, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial x} \\tanh(\\log(1+e^x)) &= (1 - \\tanh^2(\\log(1+e^x))) \\frac{1}{1+e^x} e^x\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we apply the product rule of differentiation\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial x} [f(x)g(x)] = f'(x)g(x) + g'(x)f(x)\n",
    "\\end{align*}\n",
    "\n",
    "to complete the differentiation\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial x} \\textrm{mish}(x) &= (1)\\tanh(\\log(1+e^x)) + x \\dot (1 - \\tanh^2(\\log(1+e^x))) \\frac{1}{1+e^x} e^x\n",
    "\\end{align*}\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a548396",
   "metadata": {},
   "source": [
    "### EXERCISE5-TASK2 [10 marks]:\n",
    "(Adapted from Exercise 5.1, *Dive into Deep Learning*) In class, we saw how adding layers to a linear deep network (a network without a nonlinearity) can never increase the expressive power of a network. Give an example where adding layers to a linear deep network reduces the expressive power of a network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c91066",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "Adding layers with dimensionality reduction can reduce the expressive capacity of a linear network (assuming no non-linear activations between layers). Suppose $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times m} $ and $\\mathbf{W}_2 \\in \\mathbb{R}^{m \\times k}$ are the projection (weight) matrices from the input to the first hidden layer and to the first hidden layer to the second hidden layer, respectively, and $\\mathbf{b}_1 \\in \\mathbb{R}^m$ and $\\mathbf{b}_2 \\in \\mathbb{R}^k$ are vectors representing the biases on units in the first and second hidden layers, respectively. Let's assume the output, $\\mathbf{Y}$ is an affine function of activities in the second hidden layer:\n",
    "\n",
    "\\begin{align*} \\mathbf{Y} &= \\left( \\mathbf{X} \\mathbf{W}_{1} + \\mathbf{b}_{1} \\right) \\mathbf{W}_{2} + \\mathbf{b}_{2} \\\\\n",
    "&= \\mathbf{X} \\mathbf{W}_{1} \\mathbf{W}_{2} + \\mathbf{b}_{1} \\mathbf{W}_{2} + \\mathbf{b}_{2} \\\\\n",
    "&= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \n",
    "\\end{align*}\n",
    "\n",
    "where $ \\mathbf{W} = \\mathbf{W}_{1} \\mathbf{W}_{2} $ represents the combined effect of all weight matrices, and $\\mathbf{b}$ is the effective bias.\n",
    "\n",
    "Since we are considering a linear transformation without any intermediate non-linear activations, if the weight matrix $ \\mathbf{W}_{2} $ has fewer columns than $ \\mathbf{W}_{1} $, that is, if $k < m$, the overall transformation $ \\mathbf{W} $ effectively operates in a reduced-dimensional subspace of the input space. This limitation arises because a linear transformation with reduced dimensionality restricts the modelâ€™s ability to capture complex transformations, thereby reducing the expressive capacity.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838627e",
   "metadata": {},
   "source": [
    "## Exercise 6. AdaBoost as Fitting an Additive Model\n",
    "\n",
    "Our in-class discussion of AdaBoost did not provide a theoretical rationale for why it works.\n",
    "The AdaBoost algorithm is in fact a specific case of *forward stagewise additive modeling*, which in turn is a way of fitting an *expansion set* of elementary *basis functions*. \n",
    "\n",
    "More generally, expansion set models are central to many advanced learning techniques and take the form\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{m-1}^M \\alpha_m g_m(x, \\theta_m),\n",
    "\\end{align}\n",
    "where $M$ is the number of basis functions, $\\alpha_m$ is the weight of the $m^{th}$ classifier, and $\\theta$ are its parameters. \n",
    "\n",
    "Such models are usually fit by minimizing some loss function $l$ over the training data to find estimates of the set of importance coefficients $\\{ \\hat \\alpha_m\\}$ and basis function parameters $\\{ \\hat \\theta_m\\}$. This is to say that\n",
    "\\begin{align}\n",
    "\\{ \\hat \\alpha_m, \\hat \\theta_m\\}_1^M = \\textrm{argmin}_{\\{ \\alpha_m, \\theta_m\\}_1^M}\\sum_{i=1}^N l\\Bigg(y_i,\\sum_{m-1}^M \\alpha_m g_m(x, \\theta_m)\\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "Forward stagewise additive modelling aproaches, like AdaBoost, instead find an approximate solution to this objective by adding basis functions in sequence. That is, at each iteration $m$, one estimates\n",
    "\\begin{align}\n",
    "\\hat \\alpha_m, \\hat \\theta_m\\ = \n",
    "\\textrm{argmin}_{ \\alpha_m, \\theta_m\\ }\\sum_{i=1}^N l\\Bigg(y_i, f_{m-1} + \\alpha_m g_m(x, \\theta_m)\\Bigg)\n",
    "\\end{align}\n",
    "which positions one to make predictions for $x$\n",
    "\\begin{align}\n",
    "f_m(x) = f_{m-1}(x) + \\alpha_m g_m(x, \\theta_m)\n",
    "\\end{align}\n",
    "In the case of applying AdaBoost to learn a binary classifier, these basis functions would be the base classifiers or members of the ensemble.\n",
    "\n",
    "### EXERCISE6-TASK1 [20 marks]:\n",
    "Show how the multiplicative weight update of AdaBoost\n",
    "\\begin{align}\n",
    "w_i^{(m+1)} = w_i^{(m)}e^{\\alpha_m \\mathbb{1}(y_i \\neq g_m(x_i))}\n",
    "\\end{align}\n",
    "can be derived from the forward stagewise additive modelling technique for the special case of minimizing an exponential loss function\n",
    "\\begin{align}\n",
    "l(y,f(x)) = \\sum_{i=1}^N e^{-y_if(x_i)}\n",
    "\\end{align}\n",
    "where $y \\in \\{ -1, +1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9057425",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "See PRML Section 14.3.1\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
